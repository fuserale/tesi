%%%%%%%%%%%%%%%%%%%%%  input settings  %%%%%%%%%%%%%%%%%%

	net.hyperParam.sizeFmInput = [32 32]; 	% size of input feature map
	net.hyperParam.numFmInput  = 3;        	% number of feature map of the input

%%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%

	net.layers{end+1}.properties = struct('type',2, 'numFm',16 , 'Activation',@Relu, 'dActivation',@dRelu, 'kernel',5, 'pooling',2);
	net.layers{end+1}.properties = struct('type',2, 'numFm',32 , 'Activation',@Relu, 'dActivation',@dRelu, 'kernel',7, 'pad',1, 'dropOut',0.8);
	net.layers{end+1}.properties = struct('type',1, 'numFm',128, 'Activation',@Relu, 'dActivation',@dRelu, 'dropOut',0.8);
	net.layers{end+1}.properties = struct('type',1, 'numFm',10);

%%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%

	net.hyperParam.trainLoopCount = 1000;		%on how many images to train before evaluating the network
	net.hyperParam.testImageNum   = 2000;   	% after each loop, on how many images to evaluate network performance
	net.hyperParam.ni_initial     = 0.0001;	% ni to start training process
	net.hyperParam.ni_final       = 0.000001;	% final ni to stop the training process
	net.hyperParam.momentum       = 0.9;
    net.hyperParam.lambda         = 0.008; 		%L2 regularization factor
	net.hyperParam.flipImage      = 1;          % fill randomly flip the input hor/vert before passing to the network. Improves learning in some instances
	net.runInfoParam.verifyBP     = 0;       
	




